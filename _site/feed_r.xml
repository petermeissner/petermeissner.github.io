<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>petermeissner</title>
    <description>petermeissner: personal blog, stats, tech, and things between and beyond</description>
    <link></link>
    <atom:link href="/feed_r.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Robotstxt Update - v0.6.0 on CRAN</title>
        <description>&lt;p&gt;I just got the news from the CRAN-team that the robotstxt version update 0.6.0 
was accepted and now is available.&lt;/p&gt;

&lt;p&gt;The robotstxt package aims at working with robots.txt files from within R by 
providing a parser as well as a permission checker - and some convenience 
goodies working behind the scenes.&lt;/p&gt;

&lt;p&gt;This new version switches the default checking backend from an pure R 
implementation (done by myself) to a C++ implementation (done by &lt;a href=&quot;https://github.com/seomoz/rep-cpp&quot;&gt;Moz&lt;/a&gt; and wrapped by &lt;a href=&quot;https://github.com/hrbrmstr/spiderbar&quot;&gt;Bob Rudis&lt;/a&gt;) that is both executing much faster 
and also much more rigourous in interpreting the the standard (&lt;a href=&quot;http://www.robotstxt.org/norobots-rfc.txt&quot;&gt;RFC&lt;/a&gt;). While it was hard to throw 
away so much of my own work it also is very liberating to now have to 
maintain lesser code and leaving the package with the feeling that it now is 
in a better state (more robust) than before.&lt;/p&gt;

&lt;p&gt;Thanks CRAN, thanks Bob, thanks Moz.&lt;/p&gt;
</description>
        <pubDate>Sun, 11 Feb 2018 14:16:01 +0100</pubDate>
        <link>/blog/2018/02/11/robotstxt-v0.6.0/</link>
        <guid isPermaLink="true">/blog/2018/02/11/robotstxt-v0.6.0/</guid>
      </item>
    
  </channel>
</rss>
